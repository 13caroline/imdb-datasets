# Managing large data sets

## Project 1 ([Technical report](https://github.com/13caroline/GGCD/blob/main/Projetos/TP1/GGCD___TP1.pdf))

This practical work consisted in the implementation and experimental evaluation of data storage and processing tasks, through the development of methods and classes that extend the **Map Reduce** and **Avro + Parquet** interfaces existing in the _Apache Hadoop_ framework, allowing to answer the questions raised. In order to answer these questions, the public <a href="https://www.imdb.com/interfaces/">IMDB dataset</a> was used. 

## Project 2 ([Technical report](https://github.com/13caroline/GGCD/blob/main/Projetos/TP2/IMDB_Spark.pdf))

This practical work consisted in the implementation and experimental evaluation of data storage and processing tasks, using the **Spark** library and the **Hive Metastore** service. In order to answer these questions, the public IMDB dataset was used. 

## Collaborators

| Name            	|
|-----------------	|
| [Bruno Veloso](https://github.com/brunocv)   |
| [Carolina Cunha](https://github.com/13caroline)  	|
| [Diogo Tavares](https://github.com/diogotava) |
| [Hugo Nogueira](https://github.com/Nogats) |
| [LuÃ­s Abreu](https://github.com/luisabreu102030) |

> <img src="https://seeklogo.com/images/U/Universidade_do_Minho-logo-CB2F98451C-seeklogo.com.png" align="left" height="48" width="48" > University of Minho, Software Engineering (4th Year).
